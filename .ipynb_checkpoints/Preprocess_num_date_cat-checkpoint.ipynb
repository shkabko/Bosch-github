{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocessing\n",
    "1. Loading files\n",
    "2. Concateneations of train and test data\n",
    "3. Unduplication "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import skew, boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas.core.common import array_equivalent\n",
    "import pickle\n",
    "import gc\n",
    "from datetime import datetime\n",
    "\n",
    "def timer(start_time=None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        tmin, tsec = divmod((datetime.now() - start_time).total_seconds(), 60)\n",
    "        print(' Time taken: %i minutes and %s seconds.' %\n",
    "              (tmin, round(tsec, 2)))\n",
    "\n",
    "def duplicate_columns(frame):\n",
    "    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n",
    "    dups = []\n",
    "\n",
    "    for t, v in groups.items():\n",
    "\n",
    "        cs = frame[v].columns\n",
    "        vs = frame[v]\n",
    "        lcs = len(cs)\n",
    "\n",
    "        for i in range(lcs):\n",
    "            ia = vs.iloc[:,i].values\n",
    "            for j in range(i+1, lcs):\n",
    "                ja = vs.iloc[:,j].values\n",
    "                if array_equivalent(ia, ja):\n",
    "                    dups.append(cs[i])\n",
    "                    break\n",
    "\n",
    "    return dups\n",
    "\n",
    "print('Starting!')\n",
    "\n",
    "#################################################################################\n",
    "####  Numeric data ###\n",
    "################################\n",
    "train_chunks = pd.read_csv(\"train_numeric.csv\", index_col=0)\n",
    "train = train_chunks.drop(['Response'], axis=1)\n",
    "test = pd.read_csv(\"test_numeric.csv\", index_col=0)\n",
    "train_test = pd.concat([train, test])\n",
    "print('Concatenation OK!')\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "del(test, train)\n",
    "\n",
    "\n",
    "x_train = train_test.iloc[:ntrain, :]\n",
    "x_test = train_test.iloc[ntrain:, :]\n",
    "\n",
    "train_labels = np.array(train_chunks['Response'])\n",
    "train_ids = x_train.index\n",
    "test_ids = x_test.index\n",
    "col_train = x_train.columns\n",
    "col_test = x_test.columns\n",
    "train_df = pd.DataFrame(data=x_train, index=train_ids, columns=col_train)\n",
    "test_df = pd.DataFrame(data=x_test, index=test_ids, columns=col_test)\n",
    "labels_y = pd.DataFrame(data=train_labels,index=train_ids,columns=['Response'])\n",
    "\n",
    "print(train_df.shape,test_df.shape,labels_y.shape)\n",
    "\n",
    "train_df.to_pickle('As_is/train_num_df.pkl')\n",
    "test_df.to_pickle('As_is/test_num_df.pkl')\n",
    "labels_y.to_pickle('As_is/labels_y.pkl')\n",
    "\n",
    "del(train_df,test_df,labels_y, x_train, x_test,)\n",
    "gc.collect()\n",
    "\n",
    "print('ok!')\n",
    "#################################################################################\n",
    "#### date data ###\n",
    "################################\n",
    "\n",
    "train = pd.read_csv(\"train_date.csv\", index_col=0,)\n",
    "test = pd.read_csv(\"test_date.csv\", index_col=0)\n",
    "train_test = pd.concat([train, test])\n",
    "print('Concatenation OK!')\n",
    "\n",
    "print(train_test.shape)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "\n",
    "# takes long time!\n",
    "#dups = duplicate_columns(train_test)\n",
    "#train_test = train_test.drop(dups, axis=1)\n",
    "#print('Unduplication OK!')\n",
    "#print(train_test.shape)\n",
    "\n",
    "x_train = train_test.iloc[:ntrain, :]\n",
    "x_test = train_test.iloc[ntrain:, :]\n",
    "\n",
    "train_ids = x_train.index\n",
    "test_ids = x_test.index\n",
    "col_train = x_train.columns\n",
    "col_test = x_test.columns\n",
    "train_df = pd.DataFrame(data=x_train, index=train_ids, columns=col_train)\n",
    "test_df = pd.DataFrame(data=x_test, index=test_ids, columns=col_test)\n",
    "\n",
    "train_test = pd.concat([train_df, test_df])\n",
    "train_df.to_pickle('As_is/train_date_unduplicated.pkl')\n",
    "test_df.to_pickle('As_is/test_date_unduplicated.pkl')\n",
    "train_test.to_pickle('As_is/train_test_date_unduplicated.pkl')\n",
    "\n",
    "del(train_df,test_df, x_train, x_test)\n",
    "gc.collect()\n",
    "\n",
    "#######################################################################\n",
    "### Now categoricals\n",
    "\n",
    "train = pd.read_csv(\"train_categorical_withoutT.csv\", index_col=0)\n",
    "train.columns = train.columns.str.replace('_','__')\n",
    "test = pd.read_csv(\"test_categorical_withoutT.csv\", index_col=0)\n",
    "test.columns = test.columns.str.replace('_','__')\n",
    "train_test = pd.concat([train, test])\n",
    "print('Concatenation OK!')\n",
    "\n",
    "print(train_test.shape)\n",
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "del(test, train)\n",
    "\n",
    "\n",
    "dups = duplicate_columns(train_test)\n",
    "train_test = train_test.drop(dups, axis=1)\n",
    "print('Unduplication OK!')\n",
    "print(train_test.shape)\n",
    "\n",
    "\n",
    "train = train_test.iloc[:ntrain, :]\n",
    "test = train_test.iloc[ntrain:, :]\n",
    "train_ids = train.index\n",
    "test_ids = test.index\n",
    "col_train = train.columns\n",
    "col_test = test.columns\n",
    "train_df = pd.DataFrame(data=train, index=train_ids, columns=col_train)\n",
    "test_df = pd.DataFrame(data=test, index=test_ids, columns=col_test)\n",
    "\n",
    "train_df.to_pickle('As_is/train_cat_df.pkl')\n",
    "test_df.to_pickle('As_is/test_cat_df.pkl')\n",
    "\n",
    "del(train_df,test_df)\n",
    "gc.collect()\n",
    "print('ok!')\n",
    "###########################\n",
    "##Now make train all\n",
    "\n",
    "pkl_file1 = open('As_is/train_cat_df.pkl', 'rb')\n",
    "train_cat = pickle.load(pkl_file1)\n",
    "pkl_file2 = open('As_is/train_date_unduplicated.pkl', 'rb')\n",
    "train_date = pickle.load(pkl_file2)\n",
    "pkl_file3 = open('As_is/train_num_df.pkl', 'rb')\n",
    "train_num = pickle.load(pkl_file3)\n",
    "X = pd.concat([train_num, train_date, train_cat], axis=1)\n",
    "X.to_pickle('As_is/train_all_df.pkl')\n",
    "print('Shape of the train')\n",
    "print(X.shape)\n",
    "del(train_date, train_cat, train_num, X)\n",
    "gc.collect()\n",
    "\n",
    "######################\n",
    "## Now test all\n",
    "\n",
    "pkl_file1 = open('As_is/test_cat_df.pkl', 'rb')\n",
    "train_cat = pickle.load(pkl_file1)\n",
    "pkl_file2 = open('As_is/test_date_unduplicated.pkl', 'rb')\n",
    "train_date = pickle.load(pkl_file2)\n",
    "pkl_file3 = open('As_is/test_num_df.pkl', 'rb')\n",
    "train_num = pickle.load(pkl_file3)\n",
    "X = pd.concat([train_num, train_date, train_cat], axis=1)\n",
    "X.to_pickle('As_is/test_all_df.pkl') # Whole test\n",
    "print('Shape of the test')\n",
    "print(X.shape)\n",
    "del(train_date, train_cat, train_num, X)\n",
    "gc.collect()\n",
    "\n",
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
